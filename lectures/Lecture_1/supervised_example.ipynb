{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c33f1f-8561-4d33-8778-c516a2996239",
   "metadata": {},
   "source": [
    "# Learning from Big Data: Module 1 - Natural Language Processing\n",
    "\n",
    "#### Session 1 - Getting familiar with Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fe78d477-c18c-4275-a346-37ffb375c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc13fc-249f-4105-9b8b-07f40344feb4",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "For this example, we will use the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f580c-79ca-47d2-8075-aaf521441dcf",
   "metadata": {},
   "source": [
    "### 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c043381d-0bc6-47d9-b33b-5990e0645e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "\n",
      "Target names:\n",
      " ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# Here, we load the breast cancer dataset from sklearn\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Printing the feature names\n",
    "print(f\"Feature names:\\n {data.feature_names}\\n\")\n",
    "\n",
    "# Printing the target names (of response variable)\n",
    "print(f\"Target names:\\n {data.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3667e-b67b-4679-9e07-a3a978cf789d",
   "metadata": {},
   "source": [
    "#### Extracting the features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "082ed5ed-e1a9-4d04-b7b8-3aaf26a8d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 569 observations (rows)\n",
      "There are a total of 30 features (columns) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 133.5 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The features are stored in data.data\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Printing the dimensions:\n",
    "print(f\"There are a total of {X.shape[0]} observations (rows)\")\n",
    "print(f\"There are a total of {X.shape[1]} features (columns) \\n\")\n",
    "\n",
    "# First 5 rows of the features:\n",
    "#print(f\"{X.head()}\\n\")\n",
    "\n",
    "# Printing information:\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb57ccb-0b13-4be2-b541-73e869ee10aa",
   "metadata": {},
   "source": [
    "#### Extracting the target variable from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "24c832ee-329c-454e-88d8-f8c04b9dd017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['malignant' 'benign']\n",
      "\n",
      "There are a total of 569 observations (rows)\n",
      "\n",
      "   target\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at the target variable: ['malignent', 'benign'] -> [0, 1]\n",
    "print(f\"Classes: {data.target_names}\\n\")\n",
    "\n",
    "# The response variable is stored in data.target\n",
    "y = pd.DataFrame(data.target, columns=[\"target\"])\n",
    "\n",
    "# Printing the total number of observations:\n",
    "print(f\"There are a total of {y.shape[0]} observations (rows)\\n\")\n",
    "\n",
    "# First 5 observations:\n",
    "print(f\"{y.head()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc07b6-238b-4606-9ab2-17c7f04bcd4a",
   "metadata": {},
   "source": [
    "### 2. Prepping the data: Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e373c9c0-07ab-4c61-87db-23145e0771bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases for benign: 357\n",
      "Number of cases for malignent: 212\n"
     ]
    }
   ],
   "source": [
    "# Combining the X and y datasets\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Separating majority and minority classes\n",
    "df_majority = df[df[\"target\"]==1]\n",
    "df_minority = df[df[\"target\"]==0]\n",
    "\n",
    "# The two classes have a large difference in the nubmer of cases\n",
    "print(f\"Number of cases for benign: {df_majority.shape[0]}\")\n",
    "print(f\"Number of cases for malignent: {df_minority.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1674a25a-eb09-4845-a299-7e8cedac1c0d",
   "metadata": {},
   "source": [
    "**Note** that the two classes have a large difference in the number of cases (357 versus 212). \n",
    "The author of this example dataset and code suggest using \"down sample\" or \"up sample\" to \n",
    "have the same number of cases. \n",
    "However, this can be a problematic approach. There are other approaches in the literature.\n",
    "Furthermore, over-representation and under-representation of a group in a dataset is an\n",
    "active area of research within the \"fairness in machine learning (FML)\" field, where the impact of\n",
    "underrepresented minorities in training is a source of concern. For more and better solutions I recommend\n",
    "exploring that FML literature and methods. This could also be a nice research question for your final assignment in module 1.\n",
    "For now, I am keeping the original author's proposed method below (i.e., down sample) but please be aware of the statistical and fairness issues mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "25f70a39-8953-4d7f-83fd-c6126fafc3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    212\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "target\n",
      "0    154\n",
      "1    142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Downsampling majority class\n",
    "df_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=42)\n",
    "\n",
    "# Combining minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Displaying new class counts\n",
    "print(f\"{df_downsampled['target'].value_counts()}\\n\")\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "train_data, test_data = train_test_split(df_downsampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# Class distribution of train data\n",
    "print(f\"{train_data['target'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e10f32-60ae-4495-b667-44efa3a8bbc2",
   "metadata": {},
   "source": [
    "### 3. Building and Fitting a Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d2c610e2-3b30-46d2-82ea-e25e143c1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199170\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:                  296\n",
      "Model:                          Logit   Df Residuals:                      292\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 04 Jul 2023   Pseudo R-squ.:                  0.7123\n",
      "Time:                        20:31:29   Log-Likelihood:                -58.954\n",
      "converged:                       True   LL-Null:                       -204.93\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.500e-63\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "mean radius        8.3179      1.562      5.327      0.000       5.257      11.378\n",
      "mean texture      -0.2163      0.065     -3.318      0.001      -0.344      -0.089\n",
      "mean perimeter    -1.4276      0.250     -5.718      0.000      -1.917      -0.938\n",
      "intercept         17.7707      2.705      6.571      0.000      12.470      23.072\n",
      "==================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.10 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defining the feature columns\n",
    "feature_cols = [\"mean radius\", \"mean texture\", \"mean perimeter\"]\n",
    "\n",
    "# Adding an intercept to the feature data\n",
    "train_data['intercept'] = 1\n",
    "test_data['intercept'] = 1\n",
    "\n",
    "# Defining the logistic regression model\n",
    "logitmod = sm.Logit(train_data[\"target\"], train_data[feature_cols + ['intercept']])\n",
    "\n",
    "# Fitting the model\n",
    "result = logitmod.fit()\n",
    "\n",
    "# Sumary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eea5d-7756-4484-9677-b9d4ba79ffe5",
   "metadata": {},
   "source": [
    "### 4. Making the prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a7d1ce20-c992-4cb3-ad6d-0ae67129f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "pred = result.predict(test_data[feature_cols + ['intercept']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f3d42-e3c8-4238-8853-fe2fd876873e",
   "metadata": {},
   "source": [
    "#### The prediction is continuous, we now discretize into a binary variable (benign or malignant).\n",
    "**What is the pros and cons of doing this:**\n",
    "+ From a machine learning development point of view? \n",
    "+ From a patient/clinician point of view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1a656067-05ff-4c7c-a787-2704958d59a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90625\n"
     ]
    }
   ],
   "source": [
    "# Converting continuous predictions to binary predictions\n",
    "y_pred_num = (pred > 0.5).astype(int)\n",
    "\n",
    "# Accuracy of approximately 91%\n",
    "print(accuracy_score(test_data[\"target\"], y_pred_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
