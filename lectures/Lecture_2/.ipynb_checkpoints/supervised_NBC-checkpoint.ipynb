{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be72daf-49a1-4982-a012-eb45f8051f44",
   "metadata": {},
   "source": [
    "# Learning from Big Data: Module 1 - Natural Language Processing\n",
    "#### Session 2 - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ee1eb-52ee-4063-9dff-0cb3bc1b4568",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### Illustration of the implementation of sentiment and content NBC-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34be9f5-f60e-42d9-8f14-b89fcfe69805",
   "metadata": {},
   "source": [
    "# Installation\n",
    "#### Based on the data available on the Github repo: https://github.com/guiliberali/Learning-from-Big-Data-Module-1 \n",
    "+ Input files read from the local clone of the repo\n",
    "+ Output files saved in a folder next to the local clone of the Github repo (in the root folder of the project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d26733-f282-45b2-893c-e8d902e3c91b",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9c0ad-7c52-46c5-87ce-1fe3cbff63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, namedtuple\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722694ff-6414-4a0d-9683-5a4af394d4fe",
   "metadata": {},
   "source": [
    "# 1. The Naive Bayes Classifier (NBC) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5313f09-2121-4072-8c0e-e94857d97dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_sentiment(prior, corpus_in, dict_words, p_w_given_c, TOT_DIMENSIONS):\n",
    "    prior = np.array(prior)\n",
    "    vec = CountVectorizer(vocabulary=dict_words, lowercase=True)\n",
    "    word_matrix = vec.fit_transform([corpus_in]).toarray()\n",
    "\n",
    "    # Check if there are any relevant words in the review, if there are, treat them. if not, use prior.\n",
    "    if word_matrix.sum() == 0:\n",
    "        posterior = prior\n",
    "        words_ = ['']\n",
    "    else:\n",
    "        # Positions in word matrix that have words from this review\n",
    "        word_matrix_indices = np.where(word_matrix > 0)[1]\n",
    "\n",
    "        # Initializing posterior vector\n",
    "        posterior = np.zeros(TOT_DIMENSIONS)\n",
    "        vec_likelihood = np.zeros(TOT_DIMENSIONS)\n",
    "\n",
    "        # Loop around words found in review\n",
    "        for word_matrix_index in word_matrix_indices:\n",
    "            word = vec.get_feature_names_out()[word_matrix_index]\n",
    "\n",
    "            # Check if the word exists in p_w_given_c.words\n",
    "            p_w_given_c_indices = np.where(p_w_given_c.words == word)[0]\n",
    "            if p_w_given_c_indices.size > 0:\n",
    "                p_w_given_c_index = p_w_given_c_indices[0]\n",
    "                vec_likelihood = np.array([p_w_given_c.pos_likelihood[p_w_given_c_index], \n",
    "                                           p_w_given_c.neg_likelihood[p_w_given_c_index]])\n",
    "                \n",
    "                for i in range(word_matrix[0, word_matrix_index]):\n",
    "                    numerat = prior * vec_likelihood\n",
    "                    denomin = prior.dot(vec_likelihood)\n",
    "                    posterior = numerat / denomin\n",
    "\n",
    "                    if np.sum(posterior) > 1.01:\n",
    "                        raise Exception('ERROR')\n",
    "\n",
    "                    prior = np.array(posterior)\n",
    "\n",
    "        words_ = vec.get_feature_names_out()[word_matrix_indices]\n",
    "\n",
    "    return {'posterior_': posterior, 'words_': words_}\n",
    "\n",
    "\n",
    "def compute_posterior_content(prior, corpus_in, dict_words, p_w_given_c, BIGRAM, TOT_DIMENSIONS):\n",
    "    vec = CountVectorizer(vocabulary=dict_words, lowercase=True, ngram_range=(1, BIGRAM))\n",
    "    word_matrix = vec.fit_transform([corpus_in]).toarray()\n",
    "\n",
    "    # Check if there are any relevant words in the review, if there are, treat them. If not, use prior.\n",
    "    if word_matrix.sum() == 0:\n",
    "        posterior = prior\n",
    "    else:\n",
    "        # Positions in word matrix that have words from this review\n",
    "        word_matrix_indices = np.where(word_matrix > 0)[1]\n",
    "        posterior = np.zeros(TOT_DIMENSIONS)\n",
    "\n",
    "        # Loop around words found in review\n",
    "        for word_matrix_index in word_matrix_indices:\n",
    "            word = vec.get_feature_names_out()[word_matrix_index]\n",
    "            p_w_given_c_index = np.where(p_w_given_c.words == word)[0][0]\n",
    "            vec_likelihood = np.array([p_w_given_c.storyline[p_w_given_c_index], \n",
    "                                       p_w_given_c.acting[p_w_given_c_index], \n",
    "                                       p_w_given_c.visual[p_w_given_c_index]])\n",
    "\n",
    "            for i in range(word_matrix[0, word_matrix_index]):\n",
    "                numerat = prior * vec_likelihood\n",
    "                denomin = prior.dot(vec_likelihood)\n",
    "                posterior = numerat / denomin\n",
    "\n",
    "                if np.sum(posterior) > 1.01:\n",
    "                    raise Exception('ERROR')\n",
    "\n",
    "                prior = posterior\n",
    "\n",
    "    return {'posterior_': posterior}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37d021-2708-4bc0-9632-0eeb336c21c3",
   "metadata": {},
   "source": [
    "# 2. Start of the Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c28b58-e348-4f05-b463-bcc42193b93c",
   "metadata": {},
   "source": [
    "## Loading the Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9729401-7008-451c-bc6f-535e795e5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "reviews_raw = pd.read_csv('../../data/reviews/reviews_tiny.csv', encoding='ISO-8859-1')\n",
    "reviews_raw = reviews_raw[\n",
    "    ['movie_name',\n",
    "     'review_code',\n",
    "     'reviewer',\n",
    "     'review_date',\n",
    "     'num_eval',\n",
    "     'prob_sentiment',\n",
    "     'words_in_lexicon_sentiment_and_review',\n",
    "     'ratio_helpful',\n",
    "     'raters',\n",
    "     'prob_storyline',\n",
    "     'prob_acting',\n",
    "     'prob_sound_visual',\n",
    "     'full_text',\n",
    "     'processed_text',\n",
    "     'release_date',\n",
    "     'first_week_box_office',\n",
    "     'MPAA',\n",
    "     'studio',\n",
    "     'num_theaters']\n",
    "]\n",
    "\n",
    "# Setting the parameters\n",
    "PRIOR_SENT = 1/2\n",
    "PRIOR_CONTENT = 1/3\n",
    "TOT_REVIEWS = len(reviews_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890c0b9-4bcd-4869-a2b8-d29fd0d76128",
   "metadata": {},
   "source": [
    "## Loading the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed366eb-14ef-4131-9871-b9c39b667c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the storyline dictionary\n",
    "dictionary_storyline = pd.read_csv('../../data/lexicons/storyline_33k.txt')\n",
    "\n",
    "# Loading the acting dictionary\n",
    "dictionary_acting = pd.read_csv('../../data/lexicons/acting_33k.txt')\n",
    "\n",
    "# Loading the visual dictionary\n",
    "dictionary_visual = pd.read_csv('../../data/lexicons/visual_33k.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1caa66-d7ef-4a5f-8dd4-2d5cac99f557",
   "metadata": {},
   "source": [
    "#### `TODO:` Compute the word likelihoods from the 3 content dictionaries (i.e., your training data). Here, we load a list of 100 words with **fake** topic/content likelihoods and a list with 100 **fake** sentiment likelihoods.\n",
    "\n",
    "**Note** that these are just examples, and these 100-word lists are **not** to be used in your assignment, you are expected to compute the content likelihoods for all the words in the training data yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9b9564-32a0-4c1c-9b93-45df071f0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the content likelihoods for all the words in the training data...\n",
    "likelihoods_content = pd.read_csv('../../data/lexicons/example_100_fake_likelihood_content.csv')\n",
    "\n",
    "# Converting the first column to a list of strings\n",
    "lexicon_content = likelihoods_content.iloc[:, 0].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3642b-9aeb-4649-8ecd-a49947f6f0ed",
   "metadata": {},
   "source": [
    "#### `TODO:` Search for a list of sentiment words that fits your *research question*. This is available from the literature.\n",
    "\n",
    "**For example**, you may want to look at **positive** and **negative** sentiment (hence two dimensions) or you may want to look at other sentiment dimensions, such as specific **emotions** (excitement, fear, etc.). The list of 100 words with fake likelihoods for the sentiment used below is **not** to be used in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bffc514-c52e-4faf-b649-585e67133442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the sentiment likelihoods using your sentiment words list...\n",
    "likelihoods_sentiment = pd.read_csv('../../data/lexicons/example_100_fake_likelihood_sentiment.csv')\n",
    "\n",
    "# Converting the first column to a list of strings\n",
    "lexicon_sentiment = likelihoods_sentiment.iloc[:, 0].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15911ce4-18b0-4874-b08a-a7922e642a34",
   "metadata": {},
   "source": [
    "## NBC Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5366fc0-b7cf-4bd9-8f50-c2f4232c26ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentiment of review #0\n",
      "Computing sentiment of review #100\n",
      "Computing sentiment of review #200\n",
      "Computing sentiment of review #300\n",
      "Computing sentiment of review #400\n",
      "Computing sentiment of review #500\n",
      "Computing sentiment of review #600\n",
      "Computing sentiment of review #700\n",
      "Computing sentiment of review #800\n",
      "Computing sentiment of review #900\n"
     ]
    }
   ],
   "source": [
    "for review_index in range(TOT_REVIEWS):\n",
    "    if (review_index % 100 == 0):\n",
    "        print(f\"Computing sentiment of review #{review_index}\")\n",
    "        \n",
    "    prior_sent = [PRIOR_SENT, 1-PRIOR_SENT]   # Reset the prior as each review is looked at separately\n",
    "    text_review = str(reviews_raw['processed_text'].iloc[review_index])\n",
    "\n",
    "    # Pre-process the review to remove punctuation marks and numbers\n",
    "    # Note: we are not removing stopwords here (nor elsewhere - a point for improvement)\n",
    "    text_review = text_review.translate(str.maketrans('', '', string.punctuation))\n",
    "    text_review = ''.join([i for i in text_review if not i.isdigit()])\n",
    "\n",
    "    # Computing posterior probability the review is positive\n",
    "    TOT_DIMENSIONS = 2\n",
    "    sent_results = compute_posterior_sentiment(prior=prior_sent,\n",
    "                                               corpus_in=text_review,\n",
    "                                               dict_words=lexicon_sentiment,\n",
    "                                               p_w_given_c=likelihoods_sentiment,\n",
    "                                               TOT_DIMENSIONS=TOT_DIMENSIONS)\n",
    "    \n",
    "    words_sent = sent_results['words_']\n",
    "    posterior_sent = sent_results['posterior_']\n",
    "\n",
    "    # Setting the posterior sentiment in the prob_sentiment column\n",
    "    reviews_raw.loc[review_index, 'prob_sentiment'] = posterior_sent[0]\n",
    "    reviews_raw.loc[review_index, 'words_in_lexicon_sentiment_and_review'] = ' '.join(words_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3e422-58c9-40e5-b940-b95bcadf0757",
   "metadata": {},
   "source": [
    "## NBC Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1947c421-b8a9-406d-bdcd-36d9f6bf638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing content of review # 0\n",
      "Computing content of review # 100\n",
      "Computing content of review # 200\n",
      "Computing content of review # 300\n",
      "Computing content of review # 400\n",
      "Computing content of review # 500\n",
      "Computing content of review # 600\n",
      "Computing content of review # 700\n",
      "Computing content of review # 800\n",
      "Computing content of review # 900\n"
     ]
    }
   ],
   "source": [
    "for review_index in range(TOT_REVIEWS):\n",
    "    print(f'Computing content of review # {review_index}') if review_index%100 == 0 else None\n",
    "    \n",
    "    if reviews_raw['full_text'].iloc[review_index] != \"\":\n",
    "        text_review = str(reviews_raw['processed_text'].iloc[review_index])\n",
    "\n",
    "        # Pre-process the review to remove punctuation marks and numbers\n",
    "        # Note: we are not removing stopwords here (nor elsewhere - a point for improvement)\n",
    "        text_review = text_review.translate(str.maketrans('', '', string.punctuation))\n",
    "        text_review = ''.join([i for i in text_review if not i.isdigit()])\n",
    "        \n",
    "        # Compute posterior probability the review is about each topic/content\n",
    "        TOT_DIMENSIONS = 3\n",
    "        prior_content = np.repeat(PRIOR_CONTENT, TOT_DIMENSIONS).reshape(-1, TOT_DIMENSIONS)\n",
    "        posterior_content = compute_posterior_content(prior=prior_content, \n",
    "                                              corpus_in=text_review,\n",
    "                                              dict_words=lexicon_content,\n",
    "                                              p_w_given_c=likelihoods_content, \n",
    "                                              BIGRAM=2,\n",
    "                                              TOT_DIMENSIONS=TOT_DIMENSIONS)\n",
    "        \n",
    "        reviews_raw.loc[review_index, 'prob_storyline'] = posterior_content['posterior_'][0][0]\n",
    "        reviews_raw.loc[review_index, 'prob_acting'] = posterior_content['posterior_'][0][1]\n",
    "        reviews_raw.loc[review_index, 'prob_sound_visual'] = posterior_content['posterior_'][0][2]\n",
    "\n",
    "processed_reviews = reviews_raw\n",
    "\n",
    "# Save the updated file, now including the sentiment and content/topic posteriors.\n",
    "processed_reviews.to_csv('../../output/test_processed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fded7c-cc5c-48d5-8234-6461bdde8b96",
   "metadata": {},
   "source": [
    "## Performance: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dbe8615-c06d-42e3-8a61-c02cc3b08cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the judges scores\n",
    "ground_truth_judges = pd.read_csv('../../data/judges/judges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a88b8-c7ac-4cba-8068-7dda5f3f9d63",
   "metadata": {},
   "source": [
    "#### `TODO:` Compare the performance of your NBC implementation (for content) against the judges ground truth.\n",
    "\n",
    "Do this by running your algorithm on the sentences labeled by the judges and comparing your classification against the ground truth. Provide the following:\n",
    "+ The confusion matrix\n",
    "+ The model precision\n",
    "+ The accuracy score\n",
    "+ **An interpretation of your findings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "343ea4cb-d7a4-46aa-9adc-4f7f7c5f76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73187350-ae2d-4c6e-b196-eef35bf482ac",
   "metadata": {},
   "source": [
    "# 3. Vader Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e007dd-14e0-4880-93d7-ccabf5c278dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing VADER sentiment of review #0\n",
      "Computing VADER sentiment of review #100\n",
      "Computing VADER sentiment of review #200\n",
      "Computing VADER sentiment of review #300\n",
      "Computing VADER sentiment of review #400\n",
      "Computing VADER sentiment of review #500\n",
      "Computing VADER sentiment of review #600\n",
      "Computing VADER sentiment of review #700\n",
      "Computing VADER sentiment of review #800\n",
      "Computing VADER sentiment of review #900\n"
     ]
    }
   ],
   "source": [
    "# Initializing for VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for review_index in range(TOT_REVIEWS):\n",
    "    if (review_index % 100) == 0: \n",
    "        print(f\"Computing VADER sentiment of review #{review_index}\")\n",
    "\n",
    "    if reviews_raw.loc[review_index, 'full_text'] != \"\":\n",
    "        text_review = reviews_raw.loc[review_index, 'processed_text']\n",
    "\n",
    "        # Pre-process the review to remove numbers and punctuation marks.\n",
    "        text_review = re.sub(r'\\b\\w{1,2}\\b', '', text_review)\n",
    "        text_review = re.sub(r'[^a-zA-Z ]+', ' ', text_review)\n",
    "        text_review = ' '.join(text_review.split())\n",
    "\n",
    "        # Analyze sentiment\n",
    "        vader_scores = analyzer.polarity_scores(text_review)\n",
    "        reviews_raw.loc[review_index, 'vader_pos'] = vader_scores['pos']\n",
    "\n",
    "processed_reviews = reviews_raw\n",
    "processed_reviews.to_csv('../../output/VADER_processed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530c9c6-d140-4cdc-9003-84ad566f2db8",
   "metadata": {},
   "source": [
    "## Vader vs. NBC comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e66d5-bcfc-41b9-821f-f0cfcd463328",
   "metadata": {},
   "source": [
    "#### `TODO:` Compare the performance of your NBC implementation (for sentiment) assuming that the VADER classification were the ground truth and then building the confusion matrix and computing precision and recall.\n",
    "\n",
    "**Note** that we are now interested in understanding how and how much the two classifications differ (we are not implying that VADER is error-free). We are interested in uncovering sources of systemic differences that can be attributed to the algorithms or lexicons.\n",
    "+ **Do interpret your findings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0609ecd3-1a84-4daf-af94-0220de6e3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementation..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
