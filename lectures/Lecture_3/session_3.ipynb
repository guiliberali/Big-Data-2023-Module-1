{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "131605a0-28b4-4496-a0d8-3fa352514b90",
   "metadata": {},
   "source": [
    "# Learning from Big Data: Module 1 - Natural Language Processing\n",
    "\n",
    "#### Session 3 - LDA and Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef123b-3d91-488c-aa7d-5c0ba73fc46b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### This file illustrates `LDA` (Latent Dirichlet Allocation) and `Word2Vec`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ae1cf-58ef-463d-ac5d-0b9c63286f36",
   "metadata": {},
   "source": [
    "# 1. Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff65913-52bb-423d-8224-9b7d178ef776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28b4d1-a673-464d-99c7-427597c3e05e",
   "metadata": {},
   "source": [
    "# 2. Loading the Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e3623-6872-4aba-8c43-8d144254ed43",
   "metadata": {},
   "source": [
    "Next, we load the review data. **Note** that we use the ISO-8859-1 encoding from the pd.readcsv() function - this helps reading the review text correctly for further processing (by correctly interpreting non-ASCII symbols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c5747f-dbd0-479f-b2b1-0e51181ff621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the review data.\n",
    "reviews_raw = pd.read_csv('../../data/reviews/reviews_tiny.csv', encoding='ISO-8859-1')\n",
    "reviews_raw = reviews_raw[\n",
    "    ['movie_name',\n",
    "     'review_code',\n",
    "     'reviewer',\n",
    "     'review_date',\n",
    "     'num_eval',\n",
    "     'prob_sentiment',\n",
    "     'words_in_lexicon_sentiment_and_review',\n",
    "     'ratio_helpful',\n",
    "     'raters',\n",
    "     'prob_storyline',\n",
    "     'prob_acting',\n",
    "     'prob_sound_visual',\n",
    "     'full_text',\n",
    "     'processed_text',\n",
    "     'release_date',\n",
    "     'first_week_box_office',\n",
    "     'MPAA',\n",
    "     'studio',\n",
    "     'num_theaters']\n",
    "]\n",
    "\n",
    "TOT_REVIEWS = len(reviews_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a254e-f94f-4ff5-b35c-082d36361b9e",
   "metadata": {},
   "source": [
    "### 2.1 Calculating the likelihoods with your own content likelihood file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57c1deb-a6cb-4bc3-9373-105537af4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the content likelihoods for all the words in the training data...\n",
    "likelihoods_content = pd.read_csv('../../data/lexicons/example_100_fake_likelihood_content.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a9324-b3fc-4ba6-8254-41d0e8da1f2c",
   "metadata": {},
   "source": [
    "### 2.2 Inspecting the list of words to be passed for to LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1e2531-88df-4060-b729-ad9f3d688ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['story' 'hero' 'world' 'character' 'moral' 'audience' 'opponent' 'ofthe'\n",
      " 'scene' 'one' 'not' 'characters' 'plot' 'will' 'can' 'also' 'man'\n",
      " 'desire' 'stories' 'two' 'time' 'see' 'line' 'great' 'must' 'good' 'way'\n",
      " 'revelation' 'ofa' 'first' 'need' 'make' 'michael' 'change' 'house'\n",
      " 'heros' 'action' 'main' 'get' 'love' 'dialogue' 'selfrevelation' 'many'\n",
      " 'just' 'technique' 'end' 'structure' 'steps' 'tells' 'life' 'argument'\n",
      " 'symbol' 'key' 'george' 'wants' 'anatomy' 'only' 'theme' 'use' 'well'\n",
      " 'even' 'single' 'place' 'principle' 'opposition' 'comes' 'look' 'values'\n",
      " 'rick' 'storyteller' 'new' 'point' 'writers' 'big' 'web' 'within' 'says'\n",
      " 'premise' 'scenes' 'people' 'conflict' 'human' 'weakness' 'back' 'take'\n",
      " 'form' 'down' 'beginning' 'give' 'come' 'show' 'designing' 'doesnt'\n",
      " 'makes' 'king' 'three' 'example' 'family' 'plan' 'know']\n"
     ]
    }
   ],
   "source": [
    "# Converting the first column to a list of strings\n",
    "lexicon_content = likelihoods_content.iloc[:, 0].values.astype('U')\n",
    "print(lexicon_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b357f71-1381-43d7-a6e2-0220044b6fae",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning: Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a278f4-236f-413c-9a1a-21a35f10d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CountVectorizer to create the Document-Term matrix\n",
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             vocabulary={word: i for i, word in enumerate(lexicon_content)}, \n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  \n",
    "                            )\n",
    "\n",
    "# Applying the vectorizer\n",
    "data_vectorized = vectorizer.fit_transform(reviews_raw['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ff5ce-8954-4056-a208-062226011da0",
   "metadata": {},
   "source": [
    "**Next**, we will set the `LDA` parameters.\n",
    "+ `k` is the number of topic we ask LDA to estimate. In supervised learning, we set it equal to 3. In this example, we arbitrarily set `k` equal to 10.\n",
    "+ `SEED` is for replicability (i.e., obtain the same number every time the code is run).\n",
    "+ `ITER` parameter is set for the maximum number of iterations for the Expectation-Maximization algorithm used by sklearn's LDA implementation\n",
    "    + In the unlikely case you have a warning of \"no convergence\", you may increase `ITER` to 2000 or 4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b889e45e-5c31-4ff4-be91-5407b273baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the LDA parameters\n",
    "SEED = 100\n",
    "ITER = 1000\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5288bd7-e3aa-4f73-b7d2-24ff53f49998",
   "metadata": {},
   "source": [
    "#### Tip: choosing which `k` to use in LDA is a **model selection problem**.\n",
    "Typically, the best approach is to compute a model for each level of `k`, save the model log-likelihood, and choosing the `k` that produced the highest log-likelihood.\n",
    "+ The `LatentDirichletAllocation` object in sklearn has a method called `score` which returns the log-likelihood.\n",
    "+ The `score` method can be used after the model has been fitted, as follows:\n",
    "  + `loglikelihood_k = lda_model_k.score(data_vectorized)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a8818-31fa-4c28-a952-365ed198b3ce",
   "metadata": {},
   "source": [
    "**Next**, we will run the LDA and save the model. The model produced by `LatentDirichletAllocation()` is an object of class LatentDirichletAllocation (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). This class includes the topics, the score (log-likelihood), and a lot more. To extract these elements, one should use the Methods listed under \"Methods\"  in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5be2ec-0dc3-4cf8-8fa7-853c11f25db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=k,               \n",
    "                                      max_iter=ITER,\n",
    "                                      learning_method='online',\n",
    "                                      random_state=SEED,          \n",
    "                                      batch_size=128,            \n",
    "                                      evaluate_every = -1,       \n",
    "                                      n_jobs = -1,               \n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf883f-8c85-43f8-b296-6e24c7d157d2",
   "metadata": {},
   "source": [
    "#### Printing the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94565c58-045d-472a-a337-76a519f2b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log-likelihood for k = 10 is -56185.756\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = lda_model.score(data_vectorized)\n",
    "print(f\"The log-likelihood for k = {k} is {log_likelihood:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9fb78-fc28-4436-a5d9-8453b15ac661",
   "metadata": {},
   "source": [
    "#### Inspecting the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23bf556b-81d3-446d-bbe9-9bc4b41356f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  Topic8  \\\n",
      "Review_1      0.058   0.005   0.465   0.005   0.005   0.005   0.005   0.441   \n",
      "Review_2      0.003   0.003   0.272   0.003   0.003   0.003   0.106   0.412   \n",
      "Review_3      0.003   0.397   0.477   0.003   0.003   0.003   0.003   0.058   \n",
      "Review_4      0.003   0.291   0.465   0.003   0.003   0.003   0.003   0.003   \n",
      "Review_5      0.011   0.495   0.272   0.011   0.011   0.011   0.011   0.011   \n",
      "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "Review_996    0.011   0.011   0.194   0.011   0.011   0.011   0.011   0.338   \n",
      "Review_997    0.008   0.008   0.008   0.008   0.008   0.008   0.290   0.287   \n",
      "Review_998    0.006   0.435   0.006   0.006   0.006   0.006   0.355   0.168   \n",
      "Review_999    0.009   0.694   0.009   0.009   0.009   0.009   0.009   0.009   \n",
      "Review_1000   0.006   0.006   0.006   0.006   0.006   0.006   0.006   0.707   \n",
      "\n",
      "             Topic9  Topic10  \n",
      "Review_1      0.005    0.005  \n",
      "Review_2      0.003    0.189  \n",
      "Review_3      0.050    0.003  \n",
      "Review_4      0.107    0.119  \n",
      "Review_5      0.011    0.155  \n",
      "...             ...      ...  \n",
      "Review_996    0.011    0.391  \n",
      "Review_997    0.091    0.283  \n",
      "Review_998    0.006    0.006  \n",
      "Review_999    0.233    0.009  \n",
      "Review_1000   0.072    0.183  \n",
      "\n",
      "[1000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# The columns/topics names (formatting)\n",
    "topic_names = [\"Topic\" + str(i + 1) for i in range(lda_model.n_components)]\n",
    "\n",
    "# The rows/indices names (formatting)\n",
    "doc_names = [\"Review_\" + str(i + 1) for i in range(data_vectorized.shape[0])]\n",
    "\n",
    "# Posterior probabilities per document by topic\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 3), columns=topic_names, index=doc_names)\n",
    "print(df_document_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9476100f-c577-4d63-bf40-59f95552dddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  Topic8  \\\n",
      "Review_999   0.009   0.694   0.009   0.009   0.009   0.009   0.009   0.009   \n",
      "\n",
      "            Topic9  Topic10  \n",
      "Review_999   0.233    0.009  \n"
     ]
    }
   ],
   "source": [
    "# Printing the 999-th one\n",
    "print(df_document_topic[998:999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d24be6-78be-4359-9794-7188b7a10b39",
   "metadata": {},
   "source": [
    "#### Tip: for the data splits.\n",
    "For the data splits, if you can, mind the time. It's best to train on a split that temporarily precedes the prediction split, but sometimes that is not viable. However, it is good to be aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6076d49b-62b2-44e2-89ee-69af557f1bdb",
   "metadata": {},
   "source": [
    "# 4. Unsupervised Learning: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da0e97-6572-4f30-9e9e-b452fbc46b3e",
   "metadata": {},
   "source": [
    "Our word embeggind example has three steps.\n",
    "+ First, run Word2Vec to train a model using the training data split.\n",
    "+ Second, use the trained model to analyze the prediction data split.\n",
    "+ Third, use the constructed variables to forecast the `box office`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36474ed-cda8-459a-ba3d-494980826789",
   "metadata": {},
   "source": [
    "### Step 1: Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "515137e0-8f64-459a-89a2-1c1c09bb27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = reviews_raw['full_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7128549-f250-42e1-b6ca-b2e2a37152df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use a split of the data here (say 70%) instead of the entire dataset\n",
    "# train_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b3db05-a05a-48d3-8286-41bdbdba1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing each sentence into a list of words\n",
    "full_data = [simple_preprocess(line, deacc=True) for line in full_data]\n",
    "\n",
    "# Number of topics for Word2Vec\n",
    "topics_word2vec = 10\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(full_data, vector_size=topics_word2vec, sg=0, epochs=20)\n",
    "\n",
    "# The embeddings in gensim's Word2Vec model can be accessed via the 'wv' attribute\n",
    "embeddings = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c173d5c-d261-4616-b2ed-996570d6e9df",
   "metadata": {},
   "source": [
    "### Step 2: Constructing variables from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f526318-f74f-4f37-b31e-b52b46fa78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the other split of the data here (30%)\n",
    "# test_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40541a4e-be6a-49a2-9a40-f9b3775b1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the embeddings matrix\n",
    "all_embeddings = np.zeros((TOT_REVIEWS, topics_word2vec))\n",
    "\n",
    "# Looping through each review\n",
    "for review in range(TOT_REVIEWS):\n",
    "    \n",
    "    # Tokenizing the review: identify the words, separately\n",
    "    tokenized_review = simple_preprocess(reviews_raw['full_text'].iloc[review])\n",
    "\n",
    "    # Getting the word vectors per review\n",
    "    embedding_review = [] # Initializing an empty list to store the word vectors\n",
    "\n",
    "    # Looping through each word in the tokenized review\n",
    "    for word in tokenized_review:\n",
    "    \n",
    "        # Checking if the word exists in the Word2Vec model vocabulary\n",
    "        if word in model.wv.key_to_index:\n",
    "        \n",
    "            # If it does, get its vector and add it to the list\n",
    "            word_vector = model.wv[word]\n",
    "            embedding_review.append(word_vector)\n",
    "\n",
    "    # Here, we handle the case where none of the words in the review are in the Word2Vec vocabulary\n",
    "    if not embedding_review:\n",
    "        continue\n",
    "    \n",
    "    # Compute mean across all words in the review \n",
    "    all_embeddings[review, :] = np.mean(embedding_review, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2290a-127c-4dee-9946-adc1db83eb6c",
   "metadata": {},
   "source": [
    "#### Inspecting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018f433b-e5aa-4c9d-bf1a-af93dd78ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  Topic8  \\\n",
      "Review_1     -0.319  -0.674   0.766  -0.042   0.346   0.958   0.392   0.532   \n",
      "Review_2     -0.190  -0.686   0.530  -0.007   0.142   0.851   0.492   0.490   \n",
      "Review_3     -0.115  -0.913   0.390  -0.021   0.542   0.905   0.582   0.222   \n",
      "Review_4     -0.623  -0.874   0.476  -0.287   0.394   1.418   0.740   0.289   \n",
      "Review_5     -0.023  -0.788   0.304  -0.112   0.534   0.937   0.758   0.416   \n",
      "...             ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "Review_996    0.131  -0.716   0.515   0.097  -0.034   0.822   0.221   0.474   \n",
      "Review_997    0.088  -0.727   0.397   0.519   0.851   0.573   0.478   0.111   \n",
      "Review_998    0.474  -1.082  -0.132   0.019   0.633   1.190   0.553  -0.278   \n",
      "Review_999    0.508  -0.592   0.284   0.148   0.832   0.735   0.353  -0.102   \n",
      "Review_1000   0.169  -0.998   0.225   0.085   0.135   0.631   0.329   0.378   \n",
      "\n",
      "             Topic9  Topic10  \n",
      "Review_1     -0.479   -0.239  \n",
      "Review_2     -0.715   -0.225  \n",
      "Review_3     -0.713    0.006  \n",
      "Review_4     -0.606    0.225  \n",
      "Review_5     -0.718    0.292  \n",
      "...             ...      ...  \n",
      "Review_996   -0.986   -0.305  \n",
      "Review_997   -0.827   -0.018  \n",
      "Review_998   -1.000   -0.314  \n",
      "Review_999   -0.954   -0.533  \n",
      "Review_1000  -0.970   -0.482  \n",
      "\n",
      "[1000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Word embeddings per document by topic (these are not probabilities!)\n",
    "# The columns/topics names (formatting)\n",
    "topic_names_w2v = [\"Topic\" + str(i + 1) for i in range(topics_word2vec)]\n",
    "\n",
    "# The rows/indices names (formatting)\n",
    "doc_names_w2v = [\"Review_\" + str(i + 1) for i in range(all_embeddings.shape[0])]\n",
    "\n",
    "# Posterior probabilities per document by topic\n",
    "df_document_w2v_topic = pd.DataFrame(np.round(all_embeddings, 3), columns=topic_names_w2v, index=doc_names_w2v)\n",
    "print(df_document_w2v_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67b2556a-e395-43ad-be82-04f164bc33ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  Topic8  \\\n",
      "Review_999   0.508  -0.592   0.284   0.148   0.832   0.735   0.353  -0.102   \n",
      "\n",
      "            Topic9  Topic10  \n",
      "Review_999  -0.954   -0.533  \n"
     ]
    }
   ],
   "source": [
    "# Printing the 999-th one\n",
    "print(df_document_w2v_topic[998:999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa21edf-327f-4aa9-8176-df548c1d5080",
   "metadata": {},
   "source": [
    "### Step 3: Using the constructed variables to forecast the `box office`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25a7220d-e8b5-4eb2-8826-a91e045af63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementation..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
